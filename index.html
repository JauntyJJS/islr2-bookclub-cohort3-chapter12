<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Chapter 12:  Unsupervised Learning</title>
    <meta charset="utf-8" />
    <meta name="author" content="Jeremy Selva" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/panelset/panelset.css" rel="stylesheet" />
    <script src="libs/panelset/panelset.js"></script>
    <script src="libs/core-js/shim.min.js"></script>
    <script src="libs/react/react.min.js"></script>
    <script src="libs/react/react-dom.min.js"></script>
    <script src="libs/reactwidget/react-tools.js"></script>
    <script src="libs/htmlwidgets/htmlwidgets.js"></script>
    <script src="libs/reactable-binding/reactable.js"></script>
    <link rel="stylesheet" href="libs/remark.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Chapter 12: <br>Unsupervised Learning
]
.author[
### Jeremy Selva
]

---






# Introduction

This chapter introduces a diverse set of unsupervised learning techniques

- Principal Components Analysis
- Clustering

---

# 12.1 The Challenge of Unsupervised Learning

.pull-left[
Unsupervised learning is often performed as part of an exploratory data analysis.

Given a set of `\(p\)` features `\(X_1,X_2, . . . ,X_p\)` measured on `\(n\)` observations, find interesting groups about these features.

Interesting groups include
 - Finding subgroups among the variables
 - Finding subgroups among the observation
 ]
 
.pull-right[
&lt;img src="img/biclustering.jpg" width="100%" style="display: block; margin: auto;" /&gt;
]

However, there is no clear criteria to determine if a group is interesting or not as it is subjective.

Image from https://www.sciencedirect.com/science/article/pii/S1532046415001380
---

# 12.2 Principal Components Analysis (PCA)

When we have a large set of (preferably correlated) features `\(X_1,X_2,...,X_p\)`, PCA helps to summarise them into a smaller number of representative variables (called principal component) `\(Z_1,Z_2, . . . ,Z_M\)`, where `\(M &lt; p\)` that helps to explain most of the variability in the original set. `\(Z_1,Z_2,...,Z_M\)` are also called principal components.

&lt;img src="img/figure_6_14.jpg" width="70%" style="display: block; margin: auto;" /&gt;

---

# 12.2.1 What Are Principal Components?

Given a `\(n\)` by `\(p\)` data set `\(X\)`, with all `\(p\)` variable to have mean `\(0\)`,

a principal component `\(Z_j\)`, for `\(1 \le j \le M\)`, must be expressed in terms of its features `\(X_1, X_2,..., X_p\)` and loadings `\(\phi_{1j}, \phi_{2j}, ... , \phi_{pj}\)` in the following (linear combination) way

`$$Z_j = \phi_{1j}X_1 + \phi_{2j}X_2 + ... + \phi_{pj}X_p$$`

where the sum of loading squares add up to one or  `\(\sum_{k=1}^{p}(\phi_{kj})^2 = 1\)`.

&lt;img src="img/figure_6_14.jpg" width="50%" style="display: block; margin: auto;" /&gt;

---

# 12.2.1 What Are Principal Components?

These `\(M\)` loadings make up the principal component loading vector/direction, `\(\phi_{j} = (\phi_{1j}, \phi_{2j}, . . ., \phi_{pj})^{T}\)`. 

&lt;img src="img/pca_loadings.jpg" width="70%" style="display: block; margin: auto;" /&gt;

Images from [TileStats youtube video](https://www.youtube.com/watch?v=S51bTyIwxFs)

---

# 12.2.1 What Are Principal Components?

.pull-left[
The loading vector `\(\phi_{j}\)` is used to calculate the scores `\(z_{ij}\)` for `\(1 \le i \le n\)`, `\(1 \le j \le M\)` for each principal component `\(Z_j\)`.

`$$Z_j = (z_{1j},z_{2j},...,z_{ij},...,z_{nj})^T$$`
where `\(z_{ij}\)` is the sum of the products of the loadings and the individual data values

`$$z_{ij} = \sum_{k=1}^{p}(\phi_{kj}x_{ik}) = \phi_{1j}x_{i1}  + \phi_{2j}x_{i2} + ...+ \phi_{pj}x_{ip}$$`

Images from [TileStats youtube video](https://www.youtube.com/watch?v=S51bTyIwxFs)
]

.pull-right[
&lt;img src="img/scores.jpg" width="75%" style="display: block; margin: auto;" /&gt;
]

---

# 12.2.1 What Are Principal Components?

Each principal component loadings `\(\phi_{j} = (\phi_{1j}, \phi_{2j}, . . ., \phi_{pj})^{T}\)` is optimised such that

`$$\underset{{\phi_{1j},...,\phi_{pj}}}{\text{maximize}\space} \{ \frac{1}{n} \sum_{i=1}^{n}( \sum_{k=1}^{p}(\phi_{kj}x_{ik}))^2 \}  \space \text{subject to} \sum_{k=1}^{p}(\phi_{kj})^2 = 1$$` 

where `\(\sum_{k=1}^{p}(\phi_{kj}x_{ik}) = z_{ij}\)` are the scores in the principal component `\(Z_j\)`.

As the mean of the scores `\(z_{ij}\)` is `\(0\)`, `\(\frac{1}{n} \sum_{i=1}^{n}(z_{ij} - 0)^2\)` is the sample variance of the scores in the principal component `\(Z_j\)`.


&lt;img src="img/optimise.jpg" width="60%" style="display: block; margin: auto;" /&gt;

Images from [TileStats youtube video](https://www.youtube.com/watch?v=S51bTyIwxFs)

---

# 12.2.2 Another Interpretation

Principal components provide low-dimensional linear surfaces that are closest to the `\(n\)` observations.

The first principal component loading vector `\(\phi_{1} = (\phi_{11}, \phi_{21}, . . ., \phi_{p1})^{T}\)` is the line in `\(p\)`-dimensional space that is closest to the `\(n\)` observations.

&lt;img src="img/figure_6_15.jpg" width="60%" style="display: block; margin: auto;" /&gt;

---

# 12.2.2 Another Interpretation

The first two principal component loading vector `\(\phi_{1}\)` and `\(\phi_{2}\)` is the 2D plane in `\(p\)`-dimensional space that best fit the `\(n\)` observations.

&lt;img src="img/figure_12_2.jpg" width="50%" style="display: block; margin: auto;" /&gt;

---

# 12.2.2 Another Interpretation

By a special property of the loading matrix `\(\phi^-1 = \phi^T\)` and when `\(M = min(p,n-1)\)`, it is possible to express each dataset in terms of principal component scores and loadings or `\(x_{ij} = \sum_{k=1}^{p}(z_{ik}\phi_{kj})\)`

&lt;img src="img/data_in_terms_of_PC.jpg" width="80%" style="display: block; margin: auto;" /&gt;

---

# 12.2.2 Another Interpretation

However for `\(m &lt; p\)`, `\(x_{ij} \approx \sum_{k=1}^{p}(z_{ik}\phi_{kj})\)`

&lt;img src="img/figure_12_2_revised.jpg" width="60%" style="display: block; margin: auto;" /&gt;

`\(x_{ij} - \sum_{k=1}^{p}(z_{ik}\phi_{kj})\)` will get smaller as `\(M\)` gets closer to `\(min(p,n-1)\)`


---

# 12.2.3 The Proportion of Variance Explained

Variance represents how much information for a given data is loss as a result by projecting the observations onto the first few principal components. We define some parameters.

The total variance present in a data set (assuming that the variables have mean `\(0\)`) is

`$$\sum_{j=1}^{p}Var(X_j) = \sum_{j=1}^{p} (\frac{1}{n}\sum_{i=1}^{n}(x_{ij} - 0)^{2})$$`
The variance explained by the `\(m\)`th principal component is the variance of the scores (which also has the mean of `\(0\)`).

`$$\frac{1}{n}\sum_{i=1}^{n}z_{im}^2 = \frac{1}{n}\sum_{i=1}^{n}(\sum_{j=1}^{p}\phi_{jm}x_{ij})^{2}$$`
---

# 12.2.3 The Proportion of Variance Explained

&lt;img src="img/variance_definition.jpg" width="80%" style="display: block; margin: auto;" /&gt;

Images from [TileStats youtube video](https://www.youtube.com/watch?v=S51bTyIwxFs)

---

# 12.2.3 The Proportion of Variance Explained

The proportion of variance explained `\((PVE)\)` by the `\(m\)`th principal component

`$$PVE_m = \frac{Variance\space explained\space by\space the\space mth\space principal\space component\space}{Total\space variance\space present\space in\space a\space data\space set\space} = \frac{\frac{1}{n}\sum_{i=1}^{n}z_{im}^2}{\frac{1}{n}\sum_{i=1}^{n}\sum_{j=1}^{p}(x_{ij})^{2}}$$`
&lt;img src="img/proportion_of_variance.jpg" width="50%" style="display: block; margin: auto;" /&gt;

Images from [TileStats youtube video](https://www.youtube.com/watch?v=S51bTyIwxFs)

---

# 12.2.3 The Proportion of Variance Explained

The variance of the data can also be decomposed into the variance of the `\(M\)` principal components plus the mean squared error of this `\(M\)`-dimensional approximation.

`$$\underbrace{\frac{1}{n}\sum_{i=1}^{n}\sum_{j=1}^{p}(x_{ij})^{2}}_\text{Var. of data} =  \underbrace{\frac{1}{n}\sum_{i=1}^{n}\sum_{m=1}^{M}(z_{ik})^{2}}_\text{Var. of first M PCs} +
\underbrace{\frac{1}{n}\sum_{i=1}^{n}\sum_{j=1}^{p}(x_{ij}-\sum_{m=1}^{M}z_{im}\phi_{jm})^{2}}_\text{MSE of M-dimensional approximation}$$`

&lt;img src="img/variance_decomposition.jpg" width="60%" style="display: block; margin: auto;" /&gt;

Images from [TileStats youtube video](https://www.youtube.com/watch?v=S51bTyIwxFs)

---

# 12.2.3 The Proportion of Variance Explained

`$$\underbrace{\frac{1}{n}\sum_{i=1}^{n}\sum_{j=1}^{p}(x_{ij})^{2}}_\text{Var. of data} =  \underbrace{\frac{1}{n}\sum_{i=1}^{n}\sum_{m=1}^{M}(z_{im})^{2}}_\text{Var. of first M PCs} +
\underbrace{\frac{1}{n}\sum_{i=1}^{n}\sum_{j=1}^{p}(x_{ij}-\sum_{m=1}^{M}z_{im}\phi_{jm})^{2}}_\text{MSE of M-dimensional approximation}$$`

Bring `\(\underbrace{\frac{1}{n}\sum_{i=1}^{n}\sum_{j=1}^{p}(x_{ij}-\sum_{m=1}^{M}z_{im}\phi_{jm})^{2}}_\text{MSE of M-dimensional approximation}\)` to the left hand side

`$$\underbrace{\frac{1}{n}\sum_{i=1}^{n}\sum_{j=1}^{p}(x_{ij})^{2}}_\text{Var. of data} - \underbrace{\frac{1}{n}\sum_{i=1}^{n}\sum_{j=1}^{p}(x_{ij}-\sum_{m=1}^{M}z_{im}\phi_{jm})^{2}}_\text{MSE of M-dimensional approximation} =  \underbrace{\frac{1}{n}\sum_{i=1}^{n}\sum_{m=1}^{M}(z_{im})^{2}}_\text{Var. of first M PCs}$$`
---

# 12.2.3 The Proportion of Variance Explained

`$$\underbrace{\frac{1}{n}\sum_{i=1}^{n}\sum_{j=1}^{p}(x_{ij})^{2}}_\text{Var. of data} - \underbrace{\frac{1}{n}\sum_{i=1}^{n}\sum_{j=1}^{p}(x_{ij}-\sum_{m=1}^{M}z_{im}\phi_{jm})^{2}}_\text{MSE of M-dimensional approximation} =  \underbrace{\frac{1}{n}\sum_{i=1}^{n}\sum_{m=1}^{M}(z_{im})^{2}}_\text{Var. of first M PCs}$$`

Divide by `\(\underbrace{\frac{1}{n}\sum_{i=1}^{n}\sum_{j=1}^{p}(x_{ij})^{2}}_\text{Var. of data} \neq 0\)` on both side

`$$1 - \frac{\sum_{i=1}^{n}\sum_{j=1}^{p}(x_{ij}-\sum_{m=1}^{M}z_{im}\phi_{jm})^{2}}{\sum_{i=1}^{n}\sum_{j=1}^{p}(x_{ij})^{2}} = \frac{\frac{1}{n}\sum_{i=1}^{n}\sum_{m=1}^{M}(z_{im})^{2}}{\frac{1}{n}\sum_{i=1}^{n}\sum_{j=1}^{p}(x_{ij})^{2}} = \sum_{m=1}^{M} \frac{\frac{1}{n}\sum_{i=1}^{n}(z_{im})^{2}}{\frac{1}{n}\sum_{i=1}^{n}\sum_{j=1}^{p}(x_{ij})^{2}}$$`
Recall that `\(PVE_m = \frac{Variance\space explained\space by\space the\space mth\space principal\space component\space}{Total\space variance\space present\space in\space a\space data\space set\space} = \frac{\frac{1}{n}\sum_{i=1}^{n}z_{im}^2}{\frac{1}{n}\sum_{i=1}^{n}\sum_{j=1}^{p}(x_{ij})^{2}}\)`

---

# 12.2.3 The Proportion of Variance Explained

We have 

`$$1 - \frac{\sum_{i=1}^{n}\sum_{j=1}^{p}(x_{ij}-\sum_{m=1}^{M}z_{im}\phi_{jm})^{2}}{\sum_{i=1}^{n}\sum_{j=1}^{p}(x_{ij})^{2}} = \sum_{m=1}^{M} \frac{\frac{1}{n}\sum_{i=1}^{n}(z_{im})^{2}}{\frac{1}{n}\sum_{i=1}^{n}\sum_{j=1}^{p}(x_{ij})^{2}} = \sum_{m=1}^{M}PVE_m$$`
Using `\(TSS\)` as the total sum of squared elements of `\(X\)`, and `\(RSS\)` as the residual sum of squares of the M-dimensional approximation given by the `\(M\)` principal components.

$$ \sum_{m=1}^{M}PVE_m = 1 - \frac{RSS}{TSS} = R^2$$

We can interpret the cumulative PVE as the `\(R^2\)` of the approximation for `\(X\)` given by the first `\(M\)` principal components

---

# 12.2.4 More on PCA

Why do we need to scale the variables have standard deviation one ?

&lt;img src="img/figure_12_4.jpg" width="60%" style="display: block; margin: auto;" /&gt;

---

# 12.2.4 More on PCA

Each principal component loading vector is unique, up to a sign flip or 

Given `\(j\)` and `\(j'\)`, `\(\phi_{j'm} = k\phi_{jm}\)` where `\(k\)` = `\(1\)` or `\(-1\)`.

&lt;img src="img/change_of_sign_PC.jpg" width="100%" style="display: block; margin: auto;" /&gt;

---

# 12.2.4 More on PCA

How many principal components is enough ? Typically decided using the scree plot though it can be subjective.

&lt;img src="img/figure_12_3.jpg" width="60%" style="display: block; margin: auto;" /&gt;

---

# 12.3 Missing Values and Matrix Completion

It is possible for datasets to have missing values. Unfortunately, the statistical learning methods that we have seen in this book cannot handle missing values. 

We could remove data with missing values but it is wasteful.

Another way is set the missing `\(x_{ij}\)` to be the mean of the `\(j\)`th column.

Although this is a common and convenient strategy, often we can do better by exploiting the correlation between the variables by using principal components.

---

# 12.3 Missing Values and Matrix Completion

.panelset[
.panel[.panel-name[Full Data]

.pull-left[

```r
data &lt;- data.frame(
  SBP = c(-3,-1,-1, 1, 1, 3),
  DBP = c(-4,-2, 0, 0, 2, 4))
```
]
.pull-right[
<div id="htmlwidget-646ce6e706e91ce51bc1" class="reactable html-widget" style="width:auto;height:auto;"></div>
<script type="application/json" data-for="htmlwidget-646ce6e706e91ce51bc1">{"x":{"tag":{"name":"Reactable","attribs":{"data":{"SBP":[-3,-1,-1,1,1,3],"DBP":[-4,-2,0,0,2,4]},"columns":[{"accessor":"SBP","name":"SBP","type":"numeric"},{"accessor":"DBP","name":"DBP","type":"numeric"}],"defaultPageSize":6,"paginationType":"numbers","showPageInfo":true,"minRows":1,"dataKey":"c52affd09e356c757a895355cb8ca44d"},"children":[]},"class":"reactR_markup"},"evals":[],"jsHooks":[]}</script>
]
]

.panel[.panel-name[Missing Data]
.pull-left[

```r
missing_data &lt;- data.frame(
  SBP = c(-3, NA, -1, 1, 1, NA),
  DBP = c(NA, -2, 0, NA, 2, 4)
)
```
]
.pull-right[
<div id="htmlwidget-3548af68e9dfb2d17b0f" class="reactable html-widget" style="width:auto;height:auto;"></div>
<script type="application/json" data-for="htmlwidget-3548af68e9dfb2d17b0f">{"x":{"tag":{"name":"Reactable","attribs":{"data":{"SBP":[-3,"NA",-1,1,1,"NA"],"DBP":["NA",-2,0,"NA",2,4]},"columns":[{"accessor":"SBP","name":"SBP","type":"numeric"},{"accessor":"DBP","name":"DBP","type":"numeric"}],"defaultPageSize":6,"paginationType":"numbers","showPageInfo":true,"minRows":1,"dataKey":"3e2c3e3d21455b4133e1d85eb97d2eb0"},"children":[]},"class":"reactR_markup"},"evals":[],"jsHooks":[]}</script>
]
]

.panel[.panel-name[Initialisation]
Fill in the missing values with these mean values.
Mean of SBP is `\(-0.5\)`, Mean of DBP is `\(1\)`.
.pull-left[

```r
initialisation_data &lt;- data.frame(
  SBP = c(-3, -0.5, -1, 1, 1, -0.5),
  DBP = c(1, -2, 0, 1, 2, 4)
)
```
]
.pull-right[
<div id="htmlwidget-de81358b640db294a4c6" class="reactable html-widget" style="width:auto;height:auto;"></div>
<script type="application/json" data-for="htmlwidget-de81358b640db294a4c6">{"x":{"tag":{"name":"Reactable","attribs":{"data":{"SBP":[-3,-0.5,-1,1,1,-0.5],"DBP":[1,-2,0,1,2,4]},"columns":[{"accessor":"SBP","name":"SBP","type":"numeric"},{"accessor":"DBP","name":"DBP","type":"numeric"}],"defaultPageSize":6,"paginationType":"numbers","showPageInfo":true,"minRows":1,"dataKey":"99dd86cd65fdb193b6ba51bb08cdcc25"},"children":[]},"class":"reactR_markup"},"evals":[],"jsHooks":[]}</script>
]
]
]

---

# 12.3 Missing Values and Matrix Completion

.panelset[
.panel.panel[.panel-name[Iteration 1 Data]
.pull-left[

```r
initialisation_data &lt;- data.frame(
  SBP = c(-3, -0.5, -1, 1, 1, -0.5),
  DBP = c(1, -2, 0, 1, 2, 4)
)
```
]
.pull-right[
<div id="htmlwidget-4cd36bec49ce3162d468" class="reactable html-widget" style="width:auto;height:auto;"></div>
<script type="application/json" data-for="htmlwidget-4cd36bec49ce3162d468">{"x":{"tag":{"name":"Reactable","attribs":{"data":{"SBP":[-3,-0.5,-1,1,1,-0.5],"DBP":[1,-2,0,1,2,4]},"columns":[{"accessor":"SBP","name":"SBP","type":"numeric"},{"accessor":"DBP","name":"DBP","type":"numeric"}],"defaultPageSize":6,"paginationType":"numbers","showPageInfo":true,"minRows":1,"dataKey":"99dd86cd65fdb193b6ba51bb08cdcc25"},"children":[]},"class":"reactR_markup"},"evals":[],"jsHooks":[]}</script>
]
]
.panel.panel[.panel-name[Loadings Inverse]
.pull-left[

```r
initialisation_data_pca &lt;- 
  prcomp(initialisation_data)

initialisation_data_loadings &lt;- 
  initialisation_data_pca$rotation
```


```r
initialisation_data_loadings_inverse &lt;- 
  solve(initialisation_data_loadings)["PC1",,drop = FALSE]
```
]
.pull-right[
<div id="htmlwidget-f1377c58ecae75b44ba4" class="reactable html-widget" style="width:auto;height:auto;"></div>
<script type="application/json" data-for="htmlwidget-f1377c58ecae75b44ba4">{"x":{"tag":{"name":"Reactable","attribs":{"data":{".rownames":["SBP","DBP"],"PC1":[0.207591487517845,0.978215607271796],"PC2":[-0.978215607271796,0.207591487517845]},"columns":[{"accessor":".rownames","name":"","type":"character","sortable":false,"filterable":false,"rowHeader":true},{"accessor":"PC1","name":"PC1","type":"numeric","format":{"cell":{"digits":2},"aggregated":{"digits":2}}},{"accessor":"PC2","name":"PC2","type":"numeric","format":{"cell":{"digits":2},"aggregated":{"digits":2}}}],"defaultPageSize":2,"paginationType":"numbers","showPageInfo":true,"minRows":1,"dataKey":"dd9d92801eccf7e4d9eb42ef540ce0b7"},"children":[]},"class":"reactR_markup"},"evals":[],"jsHooks":[]}</script>
&lt;br&gt;
&lt;br&gt;
<div id="htmlwidget-b8c3b1eac76e35c998de" class="reactable html-widget" style="width:auto;height:auto;"></div>
<script type="application/json" data-for="htmlwidget-b8c3b1eac76e35c998de">{"x":{"tag":{"name":"Reactable","attribs":{"data":{".rownames":["PC1"],"SBP":[0.207591487517845],"DBP":[0.978215607271796]},"columns":[{"accessor":".rownames","name":"","type":"character","sortable":false,"filterable":false,"rowHeader":true},{"accessor":"SBP","name":"SBP","type":"numeric","format":{"cell":{"digits":2},"aggregated":{"digits":2}}},{"accessor":"DBP","name":"DBP","type":"numeric","format":{"cell":{"digits":2},"aggregated":{"digits":2}}}],"defaultPageSize":2,"paginationType":"numbers","showPageInfo":true,"minRows":1,"dataKey":"0e93de6f1d6422bfc1c9e47609da508b"},"children":[]},"class":"reactR_markup"},"evals":[],"jsHooks":[]}</script>
]
]
.panel.panel[.panel-name[Scores]
.pull-left[

```r
# initialisation_data_pca &lt;- 
#   prcomp(initialisation_data)

initialisation_data_scores &lt;- 
  initialisation_data_pca$x[,"PC1",drop = FALSE]
```
]
.pull-right[
<div id="htmlwidget-007da3d0c6b2d5bae1b9" class="reactable html-widget" style="width:auto;height:auto;"></div>
<script type="application/json" data-for="htmlwidget-007da3d0c6b2d5bae1b9">{"x":{"tag":{"name":"Reactable","attribs":{"data":{"PC1":[-0.518978718794612,-2.93464682181539,-1.08201135103072,0.311387231276767,1.28960283854856,2.93464682181539]},"columns":[{"accessor":"PC1","name":"PC1","type":"numeric","format":{"cell":{"digits":2},"aggregated":{"digits":2}}}],"defaultPageSize":6,"paginationType":"numbers","showPageInfo":true,"minRows":1,"dataKey":"6819971fbd1482e29274a6d73ef74c0c"},"children":[]},"class":"reactR_markup"},"evals":[],"jsHooks":[]}</script>
]
]
.panel.panel[.panel-name[Estimated Data]
.pull-left[

```r
estimated_data_1 &lt;- 
  initialisation_data_scores %*% initialisation_data_loadings_inverse
```
]
.pull-right[
<div id="htmlwidget-58f762a57d6492116f9d" class="reactable html-widget" style="width:auto;height:auto;"></div>
<script type="application/json" data-for="htmlwidget-58f762a57d6492116f9d">{"x":{"tag":{"name":"Reactable","attribs":{"data":{"SBP":[-0.107735564224679,-0.609207699080172,-0.22461634587166,0.0646413385348073,0.267710571561531,0.609207699080172],"DBP":[-0.50767308256681,-2.87071732293039,-1.05844039082349,0.304603849540086,1.26150962385021,2.87071732293039]},"columns":[{"accessor":"SBP","name":"SBP","type":"numeric","format":{"cell":{"digits":2},"aggregated":{"digits":2}}},{"accessor":"DBP","name":"DBP","type":"numeric","format":{"cell":{"digits":2},"aggregated":{"digits":2}}}],"defaultPageSize":6,"paginationType":"numbers","showPageInfo":true,"minRows":1,"dataKey":"5f1a1271350f5a1a071e9306741cc155"},"children":[]},"class":"reactR_markup"},"evals":[],"jsHooks":[]}</script>
]
]
.panel.panel[.panel-name[Objective]
.pull-left[

```r
objective_1 &lt;- 
  sum((missing_data - estimated_data_1)^2, na.rm = TRUE)
```
]
.pull-right[

```
#&gt; [1] 14.07665
```
]
]
.panel.panel[.panel-name[Iteration 2 Data]
.pull-left[

```r
data &lt;- data.frame(
  SBP = c(-3,-1,-1, 1, 1, 3),
  DBP = c(-4,-2, 0, 0, 2, 4))

missing_data &lt;- data.frame(
  SBP = c(-3, NA, -1, 1, 1, NA),
  DBP = c(NA, -2, 0, NA, 2, 4)
)

iteration_2_data &lt;- data.frame(
  SBP = c(-3, -0.61, -1, 1, 1, 0.61),
  DBP = c(-0.51, -2, 0, 0.3, 2, 4)
)
```
]
.pull-right[
<div id="htmlwidget-c8c3c3dd8aade7bd9a51" class="reactable html-widget" style="width:auto;height:auto;"></div>
<script type="application/json" data-for="htmlwidget-c8c3c3dd8aade7bd9a51">{"x":{"tag":{"name":"Reactable","attribs":{"data":{"SBP":[-3,-0.61,-1,1,1,0.61],"DBP":[-0.51,-2,0,0.3,2,4]},"columns":[{"accessor":"SBP","name":"SBP","type":"numeric","format":{"cell":{"digits":2},"aggregated":{"digits":2}}},{"accessor":"DBP","name":"DBP","type":"numeric","format":{"cell":{"digits":2},"aggregated":{"digits":2}}}],"defaultPageSize":6,"paginationType":"numbers","showPageInfo":true,"minRows":1,"dataKey":"eab9e6ef7c886bb46bcd005ee013b00c"},"children":[]},"class":"reactR_markup"},"evals":[],"jsHooks":[]}</script>
]
]
]

---

# 12.4 Clustering Methods

Clustering is to split into distinct homogeneous groups such that 
 * Observations within each group are quite similar to each other
 * Observations in different groups are quite different from each other.

In this section we focus on perhaps the two best-known
clustering approaches: 
* K-means Clustering
* Hierarchical Clustering.

---

# 12.4.1 K-Means Clustering

.pull-left[
An approach to partition a data set into K distinct, non-overlapping clusters.

* `\(i\)` be the number of observations
* `\(x_i\)` and `\(x_{i'}\)` be two different observations
* `\(j\)` be the number of features
]
.pull-right[
To measure how much two observations `\(x_i\)` and `\(x_{i'}\)` are different from each other, the squared Euclidean distance is used

`$$\sum_{j=1}^{p}(x_{ij}-x_{i'j})^2$$`
&lt;img src="img/euclidean_distance.jpg" width="100%" style="display: block; margin: auto;" /&gt;
]

---

# 12.4.1 K-Means Clustering

* `\(k\)` be the number of clusters
* `\(C_k\)` be the `\(k\)`th cluster

Sum of all of pairwise squared Euclidean distances between the observations in the `\(k\)`th cluster

`$$\sum_{i,i' \in C_k} (\sum_{j=1}^{p}(x_{ij}-x_{i'j})^2)$$`
* `\(|C_k|\)` be the number of samples in the `\(k\)`th cluster

The within-cluster variation for cluster `\(C_k\)` denote as `\(W(C_k)\)` is the mean measurement of how much the observations within the cluster `\(C_k\)` differ from each other.

`$$W(C_k) = \frac{1}{|C_k|} \sum_{i,i' \in C_k} (\sum_{j=1}^{p}(x_{ij}-x_{i'j})^2)$$`
---

# 12.4.1 K-Means Clustering

An approach to partition a data set into K distinct, non-overlapping clusters such that the sum of within-cluster variation is as small as possible.

`$$\underset{C_1,...,C_k}{\text{minimize}}\{\sum_{k=1}^{K}W(C_k)\}$$`
`$$\underset{C_1,...,C_k}{\text{minimize}}\{\sum_{k=1}^{K}\frac{1}{|C_k|} \sum_{i,i' \in C_k} (\sum_{j=1}^{p}(x_{ij}-x_{i'j})^2)\}$$`
---

# 12.4.1 K-Means Clustering

Using `\(k=3\)` and `\(j=2\)` as an example, the algorithm starts by randomly assign a cluster number/group `\(C_1,C_2\)` and `\(C_k\)` to each of the observations.

&lt;img src="img/algorithm_12_2_step1.jpg" width="60%" style="display: block; margin: auto;" /&gt;

---

# 12.4.1 K-Means Clustering
.pull-left[
We proceed with iteration 1.

For each cluster group, calculate the cluster's *centroid*. The cluster centroids are computed as the mean of the observations assigned to each cluster.

`$$\bar{x}_{k} = \frac{1}{|C_k|} \sum_{i \in C_k}{x_{i}}$$`
where `\(|C_k|\)` is the number of observations in cluster `\(C_k\)`
]

.pull-right[
&lt;img src="img/algorithm_12_2_interation1_step2a.jpg" width="60%" style="display: block; margin: auto;" /&gt;
]
---

# 12.4.1 K-Means Clustering
.pull-left[
Next, for each given observation, calculate the distance between the observation and the centroids.

Reassign the given observation to the cluster that corresponds to the shortest distance centroid.

Calculate `\(\sum_{k=1}^{K}W(C_k)\)` for that iteration.
]
.pull-right[
&lt;img src="img/algorithm_12_2_interation1_step2b.jpg" width="60%" style="display: block; margin: auto;" /&gt;
]

---

# 12.4.1 K-Means Clustering

Repeat the iteration step on the newly assigned clusters until there are minimal changes to `\(\sum_{k=1}^{K}W(C_k)\)`.

&lt;img src="img/algorithm_12_2_interation2_step2a.jpg" width="50%" style="display: block; margin: auto;" /&gt;

---

# 12.4.1 K-Means Clustering

Caveats

* We must decide how many clusters we expect in the data.
* Results obtained will depend on the initial (random) cluster assignment of each observation which may give inconsistent results.

For this reason, it is important to run the algorithm multiple times from different random initial configurations. Then one selects the best solution, i.e. that for which `\(\sum_{k=1}^{K}W(C_k)\)` is the smallest.

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="libs/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "googlecode",
"highlightLines": true,
"highlightLanguage": "r",
"countIncrementalSlides": false,
"slideNumberFormat": "<div class=\"progress-bar-container\">\n  <div class=\"progress-bar\" style=\"width: calc(%current% / %total% * 100%);\">\n  </div>\n</div>"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
